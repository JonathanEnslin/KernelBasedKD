{
    "metadata": {
      "description": "Training parameters for different experimental setups",
      "version": "1.0"
    },
    "AT_b_1000": {
      "notes": {
        "description": "Standard attention transfer setup with beta=1000"
      },
      "distillation_type": "at",
      "gamma": 1.0,
      "beta": 1000.0,
      "params": {
        "mode": "impl"
      }
    },
    "AT_b_1000_postact": {
      "notes": {
        "description": "Standard attention transfer setup with beta=1000"
      },
      "distillation_type": "at",
      "gamma": 1.0,
      "beta": 1000.0,
      "params": {
        "mode": "impl",
        "use_post_activation": true
      }
    },
    "Vanilla": {
      "notes": {
        "description": "Standard vanilla training using the hparams from the original paper by Hinton et al."
      },
      "gamma": 0.2,
      "alpha": 0.8,
      "vanilla_temperature": 4.0
    },
    "AT+Vanilla": {
      "notes": {
        "description": "A combination of vanilla training and attention transfer"
      },
      "distillation_type": "at",
      "gamma": 0.2,
      "alpha": 0.8,
      "beta": 1000.0,
      "vanilla_temperature": 4.0,
      "params": {
        "mode": "impl"
      }
  },
  "kAT_2_2": {
    "notes": {
      "description": "Filter attention transfer"
    },
    "distillation_type": "filter_at",
    "gamma": 1.0,
    "beta": 1000.0,
    "params": {
      "map_p": 2.0,
      "loss_p": 2.0
    }
  },
  "kAT_0p4_2": {
    "notes": {
      "description": "Filter attention transfer"
    },
    "distillation_type": "filter_at",
    "gamma": 1.0,
    "beta": 1000.0,
    "params": {
      "map_p": 0.4,
      "loss_p": 2.0
    }
  },
  "kAT+Vanilla": {
    "notes": {
      "description": "Filter attention transfer combined with vanilla training"
    },
    "distillation_type": "filter_at",
    "gamma": 0.2,
    "alpha": 0.8,
    "vanilla_temperature": 4.0,
    "beta": 1000.0,
    "params": {
      "map_p": 0.4,
      "loss_p": 2.0
    }
  },
  "kAT_No_Mean": {
    "notes": {
      "description": "Filter attention transfer without averaging"
    },
    "distillation_type": "filter_at",
    "gamma": 1.0,
    "beta": 1000.0,
    "params": {
      "map_p": 2.0,
      "loss_p": 2.0,
      "mean_targets": []
    }
  },
  "kAT_Mean_C_in": {
    "notes": {
      "description": "Filter attention transfer with averaging over C_in"
    },
    "distillation_type": "filter_at",
    "gamma": 1.0,
    "beta": 1000.0,
    "params": {
      "map_p": 2.0,
      "loss_p": 2.0,
      "mean_targets": ["C_in"]
    }
  },
  "kAT_Mean_C_out": {
    "notes": {
      "description": "Filter attention transfer with averaging over C_out"
    },
    "distillation_type": "filter_at",
    "gamma": 1.0,
      "beta": 1000.0,
      "params": {
        "map_p": 2.0,
        "loss_p": 2.0,
        "mean_targets": ["C_out"]
      }
    },
    "kAT_2_2_all": {
      "notes": {
        "description": "Filter attention transfer"
      },
      "distillation_type": "filter_at",
      "gamma": 1.0,
      "beta": 10.0,
      "params": {
        "map_p": 2.0,
        "loss_p": 2.0,
        "use_abs": false,
        "layer_groups": "all"
      }
    },
    "AT_Tuned": {
      "notes": {
        "description": "Attention transfer setup using post activations and hyperparameters obtained through hyperparameter tuning.\nThe configuration still specified the vanila temp for compatibility, but the alpha weight factor is set to 0."
      },
      "distillation_type": "at",
      "gamma": 1.0,
      "alpha": 0.0,
      "beta": 1401.9,
      "vanilla_temperature": 4.0,
      "params": {
        "mode": "impl",
        "use_post_activation": true
      }
    },
    "AT+Vanilla_Tuned": {
      "notes": {
        "description": "Attention transfer using post activations combined with Vanilla KD and hyperparameters obtained through hyperparameter tuning."
      },
      "distillation_type": "at",
      "gamma": 0.1568,
      "alpha": 0.8432,
      "beta": 1102.0,
      "vanilla_temperature": 4.0,
      "params": {
        "mode": "impl",
        "use_post_activation": true
      }
    },
    "Vanilla_Tuned": {
      "notes": {
        "description": "Attention transfer using post activations combined with Vanilla KD and hyperparameters obtained through hyperparameter tuning."
      },
      "distillation_type": "at",
      "gamma": 0.1568,
      "alpha": 0.8432,
      "beta": 1102.0,
      "vanilla_temperature": 4.0,
      "params": {
        "mode": "impl",
        "use_post_activation": true
      }
    }
}
  